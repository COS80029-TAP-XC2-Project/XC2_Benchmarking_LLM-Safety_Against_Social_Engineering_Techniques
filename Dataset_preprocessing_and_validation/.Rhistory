geom_line(data = df_norm, aes(x = x, y = y), color = "#0EA5E9", linewidth = 1) +
geom_vline(xintercept = outlier_cut, linetype = "dashed", linewidth = 1, color = "#DC2626") +
labs(title = "Representative Prompt Lengths (normal curve + shaded outliers)",
x = "Characters", y = "Count") +
theme_minimal(base_size = 12)
print(p_len); save_plot(p_len, "03_prompt_length_distribution", plot_idx); plot_idx <- plot_idx + 1
}
# 4a) IRA % agreement by model (Direct vs Paraphrased)
if (nrow(ira_bar) > 0) {
p_ira <- ggplot(ira_bar, aes(x = model, y = percent_agreement, fill = model)) +
geom_col(width = 0.7) +
geom_text(aes(label = paste0(round(percent_agreement,1), "%")), vjust = -0.5, size = 4) +
scale_y_continuous(labels = percent_format(scale = 1), limits = c(0, 100)) +
scale_fill_manual(values = make_hue_palette(nrow(ira_bar)), guide = "none") +
labs(title = "Direct vs Paraphrased Agreement (by Model)",
x = "Model", y = "Percent Agreement") +
theme_minimal(base_size = 12)
print(p_ira); save_plot(p_ira, "04_ira_by_model", plot_idx); plot_idx <- plot_idx + 1
}
# 4b) Inter-model agreement (PARAPHRASED)
if (nrow(inter_bar) > 0) {
p_inter <- ggplot(inter_bar, aes(x = pair, y = percent_agreement, fill = pair)) +
geom_col(width = 0.7) +
geom_text(aes(label = paste0(round(percent_agreement,1), "%")), vjust = -0.5, size = 4) +
scale_y_continuous(labels = percent_format(scale = 1), limits = c(0, 100)) +
scale_fill_manual(values = make_hue_palette(nrow(inter_bar)), guide = "none") +
labs(title = "Inter-Model Agreement on PARAPHRASED Results",
x = "Model Pair", y = "Percent Agreement") +
theme_minimal(base_size = 12)
print(p_inter); save_plot(p_inter, "05_inter_model_agreement", plot_idx); plot_idx <- plot_idx + 1
}
# 5) Duplicates per Technique â€” colourful bars
if (nrow(duplicates_by_technique) > 0) {
dup_palette <- setNames(make_hue_palette(nrow(duplicates_by_technique)),
duplicates_by_technique$Technique)
p_dup <- ggplot(duplicates_by_technique, aes(x = Technique, y = dup_rows_involved, fill = Technique)) +
geom_col(width = 0.75) +
scale_y_continuous(labels = comma) +
scale_fill_manual(values = dup_palette, guide = "none") +
labs(title = "Rows Involved in Duplicates by Technique (exact OR near-dup)",
x = "Technique", y = "Duplicate Rows") +
theme_minimal(base_size = 12) +
theme(axis.text.x = element_text(angle = 45, hjust = 1))
print(p_dup); save_plot(p_dup, "06_duplicates_by_technique", plot_idx); plot_idx <- plot_idx + 1
}
# ---------------- MINIMAL TABLES ----------------
View(schema)                            # 6) schema validation [TABLE]
View(missing_rows_prompts)              # 2) missing prompt rows + indexes [TABLE]
if (nrow(technique_counts_collapsed) > 0) View(technique_counts_collapsed)   # 1) technique balance [TABLE]
if (nrow(length_outliers) > 0) View(length_outliers)                         # 3) outlier rows [TABLE]
if (nrow(duplicates_by_technique) > 0) View(duplicates_by_technique)         # 5) dup rows per technique [TABLE]
# 4) IRA tables
if (nrow(ira_summary_Grok) > 0)   View(ira_summary_Grok)
if (nrow(ira_confusion_Grok) > 0) View(ira_confusion_Grok)
if (nrow(ira_summary_GPT5) > 0)   View(ira_summary_GPT5)
if (nrow(ira_confusion_GPT5) > 0) View(ira_confusion_GPT5)
if (nrow(ira_summary_Gemini) > 0) View(ira_summary_Gemini)
if (nrow(ira_confusion_Gemini) > 0) View(ira_confusion_Gemini)
# Optional: show split leakage pairs table (if any)
if (nrow(cross_split_near_dups) > 0) View(cross_split_near_dups)
# Return for programmatic reuse
invisible(list(
schema = schema,
missingness_strict = missingness_strict,
missing_rows_prompts = missing_rows_prompts,
technique_counts_collapsed = technique_counts_collapsed,
length_outliers = length_outliers,
duplicates_by_technique = duplicates_by_technique,
ira_bar = ira_bar,
inter_bar = inter_bar,
ira_summary_Grok = ira_summary_Grok,   ira_confusion_Grok = ira_confusion_Grok,
ira_summary_GPT5 = ira_summary_GPT5,   ira_confusion_GPT5 = ira_confusion_GPT5,
ira_summary_Gemini = ira_summary_Gemini, ira_confusion_Gemini = ira_confusion_Gemini,
cross_split_near_dups = cross_split_near_dups,
prompt_like_cols = prompt_like_cols,
rep_text_col = rep_text_col,
output_dir = output_dir
))
}
# ---- Interactive runner ----
try({
message("Select your dataset (.xlsx/.xls or .csv) ...")
path <- file.choose()
message("Reading: ", path)
df <- read_dataset_auto(path)
# make a timestamped output folder using dataset base name
stamp <- format(Sys.time(), "%Y%m%d_%H%M%S")
base  <- tools::file_path_sans_ext(basename(path))
outdir <- paste0(base, "_plots_", stamp)
message("Saving plots to: ", normalizePath(outdir, winslash = "/",
mustWork = FALSE))
results <- validate_prompts_view(
df,
rep_text_col = NULL,       # auto-pick representative prompt column
near_dup_threshold = 0.90, # near-duplicate similarity
split_leak_threshold = 0.92,
output_dir = outdir        # <--- all plots saved here
)
}, silent = FALSE)
("missingness","technique_counts","length_hist","near_dup_hist","agreement_bars"),
# validate_prompts_views.R
# ------------------------------------------------------------------
# One-file validator for prompt datasets.
# - File picker on source(); reads .xlsx/.xls (first sheet) or .csv
# - Validations open with View(); writes nothing to disk
# - Technique-based counts, missingness, near-dups, split leakage
# - Agreement (Direct vs Paraphrased) per model + inter-model agreement on PARAPHRASED
# - Console output is spaced for readability
# - Only the specific View() tables you requested are shown
# ------------------------------------------------------------------
# ---- Package setup ----
required_pkgs <- c(
"readxl","dplyr","stringr","text2vec","Matrix","readr","tools",
"irr","tibble"
)
to_install <- setdiff(required_pkgs, rownames(installed.packages()))
if (length(to_install) > 0) {
message("Installing missing packages: ", paste(to_install, collapse = ", "))
install.packages(to_install, repos = "https://cloud.r-project.org")
}
suppressPackageStartupMessages({
library(readxl)
library(dplyr)
library(stringr)
library(text2vec)
library(Matrix)
library(readr)
library(tools)
library(irr)
library(tibble)
})
# ---- Helpers ----
is_text_series <- function(x) {
if (!is.character(x)) return(FALSE)
non_null <- x[!is.na(x)]
if (length(non_null) == 0) return(FALSE)
mean_len <- mean(nchar(non_null))
whitespace_rate <- mean(stringr::str_detect(non_null, "\\s"))
(mean_len > 20) || (whitespace_rate > 0.2)
}
read_dataset_auto <- function(path) {
ext <- tolower(tools::file_ext(path))
if (ext %in% c("xlsx","xls")) {
sh <- readxl::excel_sheets(path)[1]
df <- readxl::read_excel(path, sheet = sh, .name_repair = "minimal")
df <- as.data.frame(df, stringsAsFactors = FALSE)
} else if (ext %in% c("csv","txt")) {
df <- readr::read_csv(path, show_col_types = FALSE, progress = FALSE)
df <- as.data.frame(df, stringsAsFactors = FALSE)
} else {
stop("Unsupported file type: ", ext, ". Please select .xlsx, .xls, or .csv")
}
names(df) <- trimws(names(df))
df[] <- lapply(df, function(col) if (is.factor(col)) as.character(col) else col)
df
}
.num_or_na <- function(x) { if (is.null(x) || length(x) == 0) return(NA_real_) else as.numeric(x) }
compute_agreement <- function(a, b) {
a <- as.character(a); b <- as.character(b)
keep <- !(is.na(a) | is.na(b) | a == "" | b == "")
a <- a[keep]; b <- b[keep]
if (length(a) < 2) return(list(summary=NULL, confusion=NULL))
labs <- sort(unique(c(a, b)))
ta <- factor(a, levels = labs)
tb <- factor(b, levels = labs)
tab <- as.data.frame.matrix(table(ta, tb))
agree <- sum(diag(as.matrix(tab))) / sum(as.matrix(tab))
k <- tryCatch({ irr::kappa2(cbind(as.character(ta), as.character(tb)), weight = "unweighted") }, error = function(e) NULL)
out <- data.frame(
metric = c("n_pairs","percent_agreement","cohens_kappa","kappa_se","kappa_p_value"),
value  = c(length(a),
round(100*agree,2),
round(if (!is.null(k)) .num_or_na(k$value) else NA_real_,4),
round(if (!is.null(k)) .num_or_na(k$se)    else NA_real_,4),
if (!is.null(k)) .num_or_na(k$p.value)     else NA_real_),
stringsAsFactors = FALSE
)
list(summary = out, confusion = tab)
}
guess_result_columns <- function(df) {
cols <- names(df)
res_pat <- "(label|result|decision|outcome|class|verdict|judg(e)?ment|response|policy)"
dir_pat <- "(^|[_\\-\\s])(direct|orig(inal)?)([_\\-\\s]|$)"
par_pat <- "(^|[_\\-\\s])(para(phrase|phrased)?|paraphrased|rephrase(d)?)([_\\-\\s]|$)"
res_cols <- cols[stringr::str_detect(tolower(cols), res_pat)]
dir_cols <- res_cols[stringr::str_detect(tolower(res_cols), dir_pat)]
par_cols <- res_cols[stringr::str_detect(tolower(res_cols), par_pat)]
if (length(dir_cols) == 0 || length(par_cols) == 0) {
if (length(res_cols) >= 2) {
if (length(dir_cols) == 0) dir_cols <- res_cols[1]
if (length(par_cols) == 0 && length(res_cols) >= 2) par_cols <- res_cols[2]
}
}
list(direct = unique(dir_cols)[1], paraphrase = unique(par_cols)[1])
}
# ---- Main validator ----
validate_prompts_view <- function(df,
rep_text_col = NULL,
near_dup_threshold = 0.90,
split_leak_threshold = 0.92) {
# Identify prompt-like columns
text_cols <- names(df)[vapply(df, is_text_series, logical(1))]
priority_cols <- names(df)[stringr::str_detect(names(df), regex("prompt|text|instruction|input", ignore_case = TRUE))]
prompt_like_cols <- intersect(priority_cols, text_cols)
if (length(prompt_like_cols) == 0) prompt_like_cols <- head(text_cols, 2)
if (is.null(rep_text_col)) {
if (length(prompt_like_cols) >= 1) rep_text_col <- prompt_like_cols[1]
else stop("No text-like columns found. Please ensure at least one prompt/text column is character type.")
}
# Schema & Missingness (NA-only)
schema <- data.frame(
column   = names(df),
dtype    = vapply(df, function(x) class(x)[1], character(1)),
non_null = vapply(df, function(x) sum(!is.na(x)), integer(1)),
nulls    = vapply(df, function(x) sum(is.na(x)), integer(1)),
stringsAsFactors = FALSE
)
schema$null_pct <- round(100 * schema$nulls / pmax(1, (schema$non_null + schema$nulls)), 2)
missingness <- schema %>% dplyr::select(column, null_pct) %>% dplyr::arrange(dplyr::desc(null_pct))
# Strict missingness (also used to list missing rows)
is_missing_strict <- function(v) { v <- as.character(v); v[is.na(v)] <- ""; stringr::str_trim(v) == "" }
missing_rows_long <- dplyr::bind_rows(lapply(names(df), function(col) {
v <- df[[col]]; miss <- is_missing_strict(v)
if (!any(miss)) return(NULL)
data.frame(
column = col,
row_index = which(miss) - 1,
value_preview = substr(as.character(replace(v, is.na(v), ""))[miss], 1, 200),
stringsAsFactors = FALSE
)
}))
if (is.null(missing_rows_long)) missing_rows_long <- data.frame(column=character(), row_index=integer(), value_preview=character(), stringsAsFactors = FALSE)
# Exact duplicates across main text fields
exact_duplicates <- data.frame()
if (length(prompt_like_cols) > 0) {
key <- apply(df[prompt_like_cols], 1, function(row) paste(row, collapse = " || "))
dup_mask <- duplicated(key) | duplicated(key, fromLast = TRUE)
if (any(dup_mask, na.rm = TRUE)) {
exact_duplicates <- data.frame(
row_index = which(dup_mask) - 1,
group_id  = as.integer(factor(key[dup_mask])) - 1,
preview   = substr(key[dup_mask], 1, 200),
stringsAsFactors = FALSE
) %>% dplyr::arrange(group_id, row_index)
}
}
# Near-duplicates on representative prompt column
near_duplicates <- data.frame()
if (!is.null(rep_text_col)) {
texts <- df[[rep_text_col]]; texts <- if (is.character(texts)) texts else as.character(texts); texts[is.na(texts)] <- ""
norm <- tolower(stringr::str_squish(texts))
if (length(norm) > 1 && any(nchar(norm) > 0)) {
it <- text2vec::itoken(norm, progressbar = FALSE)
vocab <- text2vec::create_vocabulary(it, ngram = c(3L, 6L), stopwords = character(0))
dtm <- text2vec::create_dtm(it, text2vec::vocab_vectorizer(vocab))
if (nrow(dtm) > 1) {
sim <- text2vec::sim2(dtm, method = "cosine", norm = "l2")
coords <- which(sim >= near_dup_threshold, arr.ind = TRUE)
coords <- coords[coords[,1] < coords[,2], , drop = FALSE]
if (nrow(coords) > 0) {
near_duplicates <- data.frame(
i = coords[,1] - 1,
j = coords[,2] - 1,
cosine_sim = sim[coords],
i_preview = substr(texts[coords[,1]], 1, 200),
j_preview = substr(texts[coords[,2]], 1, 200),
stringsAsFactors = FALSE
) %>% dplyr::arrange(dplyr::desc(cosine_sim))
}
}
}
}
# Length outliers (z > 3)
length_outliers <- data.frame()
if (length(prompt_like_cols) > 0) {
for (c in prompt_like_cols) {
s <- df[[c]]; s <- if (is.character(s)) s else as.character(s); s[is.na(s)] <- ""
nchar_vec <- nchar(s); sdv <- stats::sd(nchar_vec)
if (sdv > 0) {
z <- (nchar_vec - mean(nchar_vec)) / sdv; idx <- which(z > 3)
if (length(idx) > 0) {
length_outliers <- dplyr::bind_rows(length_outliers, data.frame(
column = c, row_index = idx - 1, char_len = nchar_vec[idx],
preview = substr(s[idx], 1, 200), stringsAsFactors = FALSE
))
}
}
}
}
# Technique coverage
category_counts <- data.frame()
technique_cols <- names(df)[stringr::str_detect(names(df), regex("\\btechnique\\b", ignore_case = TRUE))]
if (length(technique_cols) > 0) {
tech_col <- technique_cols[1]
techs <- as.character(df[[tech_col]])
category_counts <- as.data.frame(table(techs), stringsAsFactors = FALSE) %>%
dplyr::arrange(dplyr::desc(Freq)) %>% dplyr::rename(!!tech_col := techs, count = Freq)
}
# Paired-field identical (first two prompt-like columns)
paired_field_check <- data.frame()
if (length(prompt_like_cols) >= 2) {
a <- prompt_like_cols[1]; b <- prompt_like_cols[2]
A <- df[[a]]; B <- df[[b]]
A <- tolower(stringr::str_trim(replace(if (is.character(A)) A else as.character(A), is.na(A), "")))
B <- tolower(stringr::str_trim(replace(if (is.character(B)) B else as.character(B), is.na(B), "")))
eq <- sum(A == B)
paired_field_check <- data.frame(
field_A = a, field_B = b,
identical_pairs = eq,
total_pairs_checked = nrow(df),
identical_pct = round(100 * eq / max(1, nrow(df)), 2),
stringsAsFactors = FALSE
)
}
# Paraphrased exact duplicate PAIRS (counts & rows)
paraphrase_exact_duplicates <- data.frame()
paraphrase_exact_duplicate_pairs_count <- data.frame(metric = "paraphrase_exact_duplicate_pairs", value = 0)
para_text_candidates <- names(df)[stringr::str_detect(names(df), regex("para(phrase|phrased)?|paraphrased|rephrase(d)?", ignore_case = TRUE))]
para_text_col <- intersect(para_text_candidates, text_cols)
if (length(para_text_col) > 0) {
para_col <- para_text_col[1]
vals <- df[[para_col]]; vals <- if (is.character(vals)) vals else as.character(vals); vals[is.na(vals)] <- ""
key <- vals
dup_mask <- duplicated(key) | duplicated(key, fromLast = TRUE)
if (any(dup_mask, na.rm = TRUE)) {
paraphrase_exact_duplicates <- data.frame(
row_index = which(dup_mask) - 1,
group_key = key[dup_mask],
stringsAsFactors = FALSE
) %>% dplyr::mutate(group_id = as.integer(factor(group_key)) - 1) %>%
dplyr::select(row_index, group_id, group_key) %>% dplyr::arrange(group_id, row_index)
group_sizes <- table(factor(key[dup_mask]))
pairs_count <- sum(choose(as.integer(group_sizes), 2))
paraphrase_exact_duplicate_pairs_count$value <- pairs_count
}
}
# Split leakage
cross_split_near_dups <- data.frame()
split_cols <- names(df)[stringr::str_detect(names(df), regex("\\b(split|fold|partition|set)\\b", ignore_case = TRUE))]
split_status_msg <- ""
if (!is.null(rep_text_col) && length(split_cols) > 0) {
split_col <- split_cols[1]
texts <- df[[rep_text_col]]; texts <- if (is.character(texts)) texts else as.character(texts)
texts <- tolower(stringr::str_squish(replace(texts, is.na(texts), "")))
if (length(texts) > 1 && any(nchar(texts) > 0)) {
it <- text2vec::itoken(texts, progressbar = FALSE)
vocab <- text2vec::create_vocabulary(it, ngram = c(3L, 6L), stopwords = character(0))
dtm <- text2vec::create_dtm(it, text2vec::vocab_vectorizer(vocab))
if (nrow(dtm) > 1) {
sim <- text2vec::sim2(dtm, method = "cosine", norm = "l2")
coords <- which(sim >= split_leak_threshold, arr.ind = TRUE)
coords <- coords[coords[,1] < coords[,2], , drop = FALSE]
if (nrow(coords) > 0) {
splits <- as.character(df[[split_col]])
keep <- splits[coords[,1]] != splits[coords[,2]]
coords <- coords[keep, , drop = FALSE]
if (nrow(coords) > 0) {
cross_split_near_dups <- data.frame(
i = coords[,1] - 1, i_split = splits[coords[,1]],
j = coords[,2] - 1, j_split = splits[coords[,2]],
cosine_sim = sim[coords], stringsAsFactors = FALSE
) %>% dplyr::arrange(dplyr::desc(cosine_sim))
}
}
}
}
split_status_msg <- paste0("Split leakage check: split column = '", split_col,
"', threshold = ", split_leak_threshold,
", pairs found = ", nrow(cross_split_near_dups))
} else {
split_status_msg <- "Split leakage check: NO split/fold/set column found (check skipped)."
}
# Auto-detected agreement (fallback)
ira_cols <- guess_result_columns(df)
ira_summary <- data.frame(); ira_confusion <- data.frame()
if (!is.null(ira_cols$direct) && !is.null(ira_cols$paraphrase) &&
ira_cols$direct %in% names(df) && ira_cols$paraphrase %in% names(df)) {
res <- compute_agreement(df[[ira_cols$direct]], df[[ira_cols$paraphrase]])
if (!is.null(res$summary)) ira_summary <- res$summary
if (!is.null(res$confusion)) {
ira_confusion <- tibble::as_tibble(
tibble::rownames_to_column(as.data.frame(res$confusion), var = "direct\\paraphrase")
)
}
}
# Direct vs Paraphrased per model
ira_summary_Grok <- ira_confusion_Grok <- ira_summary_GPT5 <- ira_confusion_GPT5 <- ira_summary_Gemini <- ira_confusion_Gemini <- data.frame()
model_result_pairs <- list(
Grok   = list(direct = "Test Result Grok - Direct",    paraphrase = "Test Result Grok - paraphrased"),
GPT5   = list(direct = "Test Result (GPT-5) - Direct", paraphrase = "Test Result (GPT-5) - paraphrased"),
Gemini = list(direct = "Test Result Gemini - Direct",  paraphrase = "Test Result Gemini - paraphrased")
)
for (m in names(model_result_pairs)) {
cols <- model_result_pairs[[m]]
if (all(c(cols$direct, cols$paraphrase) %in% names(df))) {
res <- compute_agreement(df[[cols$direct]], df[[cols$paraphrase]])
if (!is.null(res$summary)) {
summ <- res$summary
summ <- data.frame(metric = c("model", as.character(summ$metric)),
value  = c(m, as.character(summ$value)),
stringsAsFactors = FALSE)
if (m == "Grok")   ira_summary_Grok <- summ
if (m == "GPT5")   ira_summary_GPT5 <- summ
if (m == "Gemini") ira_summary_Gemini <- summ
}
if (!is.null(res$confusion)) {
conf <- tibble::as_tibble(tibble::rownames_to_column(as.data.frame(res$confusion), var = "direct\\paraphrase"))
if (m == "Grok")   ira_confusion_Grok <- conf
if (m == "GPT5")   ira_confusion_GPT5 <- conf
if (m == "Gemini") ira_confusion_Gemini <- conf
}
}
}
# Inter-model agreement on PARAPHRASED results
inter_pairs <- list(
"Grok_vs_GPT5"   = c("Test Result Grok - paraphrased",   "Test Result (GPT-5) - paraphrased"),
"Grok_vs_Gemini" = c("Test Result Grok - paraphrased",   "Test Result Gemini - paraphrased"),
"GPT5_vs_Gemini" = c("Test Result (GPT-5) - paraphrased","Test Result Gemini - paraphrased")
)
inter_summaries <- list(); inter_confusions <- list()
for (nm in names(inter_pairs)) {
pair <- inter_pairs[[nm]]
if (all(pair %in% names(df))) {
res <- compute_agreement(df[[pair[1]]], df[[pair[2]]])
if (!is.null(res$summary)) inter_summaries[[nm]] <- res$summary %>% dplyr::mutate(pair = nm, .before = 1)
if (!is.null(res$confusion)) inter_confusions[[nm]] <- tibble::as_tibble(
tibble::rownames_to_column(as.data.frame(res$confusion), var = paste0(nm, " : left\\right"))
)
}
}
inter_summary_tbl <- if (length(inter_summaries)) dplyr::bind_rows(inter_summaries) else data.frame()
inter_confusion_tbls <- inter_confusions
# ---- Console summary (spaced) ----
cat("Rows:", nrow(df), "  Columns:", ncol(df), "\n\n")
cat("Exact duplicate rows (across main text fields):", if (nrow(exact_duplicates)>0) length(unique(exact_duplicates$row_index)) else 0, "\n")
cat("Near-duplicate pairs:", if (nrow(near_duplicates)>0) nrow(near_duplicates) else 0, "\n")
cat("Length outlier rows:", if (nrow(length_outliers)>0) length(unique(length_outliers$row_index)) else 0, "\n\n")
cat(split_status_msg, "\n\n")
cat("Paraphrased exact-duplicate PAIRS:", paraphrase_exact_duplicate_pairs_count$value, "\n\n")
if (nrow(ira_summary) > 0) cat("Auto-detected Direct vs Paraphrased agreement pairs:", ira_summary$value[ira_summary$metric=="n_pairs"], "\n\n")
if (nrow(ira_summary_Grok) > 0)   cat("Grok   (Direct vs Paraphrased) agreement pairs:",   ira_summary_Grok$value[ira_summary_Grok$metric=="n_pairs"], "\n")
if (nrow(ira_summary_GPT5) > 0)   cat("GPT-5  (Direct vs Paraphrased) agreement pairs:",   ira_summary_GPT5$value[ira_summary_GPT5$metric=="n_pairs"], "\n")
if (nrow(ira_summary_Gemini) > 0) cat("Gemini (Direct vs Paraphrased) agreement pairs:",   ira_summary_Gemini$value[ira_summary_Gemini$metric=="n_pairs"], "\n")
cat("\n")
if (nrow(inter_summary_tbl) > 0) {
cat("Inter-model agreement on PARAPHRASED results:\n")
for (nm in unique(inter_summary_tbl$pair)) {
sub <- inter_summary_tbl[inter_summary_tbl$pair == nm, , drop = FALSE]
cat(sprintf("  %s -> n=%s | %%agree=%s | kappa=%s\n",
nm,
sub$value[sub$metric=="n_pairs"],
sub$value[sub$metric=="percent_agreement"],
sub$value[sub$metric=="cohens_kappa"]))
}
cat("\n")
}
# ---- ONLY the requested View() tables ----
View(schema)
View(missingness)
View(missing_rows_long)
View(category_counts)
View(exact_duplicates)
View(near_duplicates)
View(length_outliers)
View(paired_field_check)
View(cross_split_near_dups)
View(paraphrase_exact_duplicate_pairs_count)
View(paraphrase_exact_duplicates)
View(ira_summary_Grok)
View(ira_confusion_Grok)
View(ira_summary_GPT5)
View(ira_confusion_GPT5)
View(ira_summary_Gemini)
View(ira_confusion_Gemini)
View(inter_summary_tbl)
if (length(inter_confusion_tbls) > 0) { for (nm in names(inter_confusion_tbls)) View(inter_confusion_tbls[[nm]]) }
invisible(list(
schema = schema,
missingness = missingness,
missing_rows_long = missing_rows_long,
technique_counts = category_counts,
exact_duplicates = exact_duplicates,
near_duplicates = near_duplicates,
length_outliers = length_outliers,
paired_field_check = paired_field_check,
cross_split_near_dups = cross_split_near_dups,
paraphrase_exact_duplicate_pairs_count = paraphrase_exact_duplicate_pairs_count,
paraphrase_exact_duplicates = paraphrase_exact_duplicates,
ira_summary_Grok = ira_summary_Grok,
ira_confusion_Grok = ira_confusion_Grok,
ira_summary_GPT5 = ira_summary_GPT5,
ira_confusion_GPT5 = ira_confusion_GPT5,
ira_summary_Gemini = ira_summary_Gemini,
ira_confusion_Gemini = ira_confusion_Gemini,
inter_model_paraphrase_summary = inter_summary_tbl,
inter_model_paraphrase_confusions = inter_confusion_tbls,
prompt_like_cols = prompt_like_cols,
rep_text_col = rep_text_col
))
}
# ---- Interactive runner ----
try({
message("Select your dataset (.xlsx/.xls or .csv) ...")
path <- file.choose()
message("Reading: ", path)
df <- read_dataset_auto(path)
results <- validate_prompts_view(df,
rep_text_col = NULL,
near_dup_threshold = 0.90,
split_leak_threshold = 0.92)
}, silent = FALSE)
("missingness","technique_counts","length_hist","near_dup_hist","agreement_bars"),
